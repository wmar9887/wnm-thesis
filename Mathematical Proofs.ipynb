{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06af8cd-75fd-4265-bdb8-dc0ea14b9894",
   "metadata": {},
   "source": [
    "# Block Mini Float Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802d253-c549-4683-9103-8c018e7199ca",
   "metadata": {},
   "source": [
    "### Application to BM\n",
    "\n",
    "First we identify the initial parameters of both MiniFloat (MF) and Block MiniFloat (BM).\n",
    "\n",
    "Both employ a base where, $\\beta = 2$, and for the sake of further testing the MF format in use will be an $X\\langle e, m\\rangle = X\\langle 4,3\\rangle$ where $e$ and $m$ represent the exponent and mantissa bits respectively.\n",
    "\n",
    "The equations below represent the value of a MF number. $S$ represents the value of the sign bit, and $E$ and $F$ represent the unsigned integer value of the Exponent and Fraction.\n",
    "$$\n",
    " X\\langle e, m\\rangle =\n",
    "  \\begin{cases}\n",
    "    E = 0,       & \\quad (-1)^S \\times 2^{1-\\beta_{MF}} \\times (0 + F \\times 2^{-m})  &\\text{(de-normal)}\\\\\n",
    "    Otherwise,  & \\quad (-1)^S \\times 2^{E-\\beta_{MF}} \\times (1 + F \\times 2^{-m})  &\\text{(normal)}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "From this we can see that with our $\\langle 4,3\\rangle$ format, our exponent ranges from $[0,15]$ and mantissa ranges from $[0:7]$. From the smallest subnormal value to the highest normal number gives us a range of $[2^{-9}, 480]$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed2580-a2ff-43aa-9288-560a916cfa23",
   "metadata": {},
   "source": [
    "### Theorem 1:\n",
    "Using a floating-point format with parameters $\\beta$ and $p$, and computing differences using p digits, the relative error of the result can be as large as $\\beta- 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef02e25-0329-47a9-9c8f-6212e2e23a73",
   "metadata": {},
   "source": [
    "## Proof 1: Block Minifloat \n",
    "\n",
    "Let $A$ and $B$ be two Block Minifloat tensors with shared exponent biases $\\beta^A$ and $\\beta^B$, respectively.\n",
    "\n",
    "The Block Minifloat format uses a shared exponent bias $\\beta$ for all values in a block and represents each value as $x_i=m_i⋅2^{e_i−\\beta}$, where $m_i$ is the significand and $e_i$ is the exponent.\n",
    "Assume that $\\beta^A \\gt \\beta^B$, implying that the values in $A$ have larger magnitudes on average than those in $B$.\n",
    "\n",
    "We need to show that the relative error in adding two Block Minifloat tensors can be as large as $\\beta-1$.\n",
    "\n",
    "For any value $a_i\\in A$, we have:\n",
    "$$a_i=s_A\\cdot 2^{e_A−β_A}$$\n",
    "\n",
    "Similarly, for any value $b_j\\in B$, we have:\n",
    "$$b_j=s_B\\cdot 2^{e_B−\\beta_B}$$\n",
    "\n",
    "To add $a_i$ and $b_j$, their exponents need to be aligned. This requires shifting the significand of one of the values.\n",
    "\n",
    "Exponent Alignment: Without loss of generality, assume that $\\beta_A\\gt \\beta_B$. To align the exponents, all values in $B$ must be scaled by $2^{−(\\beta_A−\\beta_B)}$. This reduces the precision of the values in $B$ because shifting the significand results in a loss of lower bits.\n",
    "\n",
    "Error Introduction:\n",
    "The scaling introduces rounding errors. The values in $B$ can lose significant precision, and their contribution to the sum becomes negligible compared to the values in $A$. This is analogous to the case described in Goldberg's Theorem 1, where subtraction between numbers with vastly different magnitudes can result in large relative errors.\n",
    "\n",
    "Relative Error:\n",
    "Let the exact sum be $S_{exact}=a_i+b_j$, and let the computed sum (with rounding) be $S_{computed}$. The relative error is given by:\n",
    "$$\\text{Relative Error} =\\frac{|S_{exact}−S_{computed}|}{|S_{exact}|}$$\n",
    "\n",
    "When $\\beta_A -\\beta_B$ is large, $b_j$'s contribution to the sum is effectively lost, leading to:\n",
    "$$S_{computed} \\approx a_i$$\n",
    "$$\\therefore \\text{Relative Error} = \\frac{|b_j|}{|a_i + b_j|} \\lt \\beta - 1$$\n",
    "\n",
    "If $|a_i|$ is very small compared to $|b_j|$ due to shared exponential difference, then the relative error can be as large as $\\beta-1$. Which in the continued use case of a $\\beta = 2$, holds true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0ff5e1b8-857a-40ac-a7f6-5d619d7ca658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999990000099999\n"
     ]
    }
   ],
   "source": [
    "a = 0.0001\n",
    "b = 10\n",
    "c = b/(a + b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75334f5-2c59-45c8-b04f-f7aaf9ffdfe1",
   "metadata": {},
   "source": [
    "consider tensor of block minifloat as an array of numbers that span a region, when the two tensors have the same exponent, their regions should overlap simply. However when there is a difference of one, these regions have a region of inaccuracy (i.e partial overlap). This means that not all information is lost, as there is a region where two numbers are shared between the tensors, and thus calcuble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f23af0e-2bc8-4d32-8e9a-6a1d49c377cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
